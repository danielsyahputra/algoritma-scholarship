---
title: 'Classification in Machine Learning 2: In-class'
author: "Inayatus Sholikhah"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_float:
      collapsed: true
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

options(scipen = 999)
```

```{=html}
<style>
body {
text-align: justify}
</style>
```
# Mindmap

```{r, out.width = "100%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("img/C2.png")
```

# üìë Naive Bayes Classifier

Naive Bayes adalah suatu algoritma klasifikasi yang didasari oleh
**Bayes' Theorem of Probability**. Faktanya, teorema Bayes amat sering
digunakan pada kehidupan sehari-hari.

## üìå Theory of Probability

Saat kita menghitung peluang 2 atau lebih kejadian terjadi bersamaan,
kita dapat menghitungnya dengan 2 cara:

-   **Independent Event**: Peluang kejadian A tidak mempengaruhi peluang
    kejadian B. Contoh:

    -   Peluang dadu keluar angka 4 pada lemparan pertama **dan**
        peluang keluar angka 6 pada lemparan kedua
    -   Peluang saya menaiki busway ke kantor dan peluang terjadi banjir
        di Tangerang
    -   Peluang student Algoritma membeli jumpstart coffee dan peluang
        instruktor Algoritma membeli chicken karage

Peluang 2 kejadian independen yang dapat terjadi secara bersamaan adalah
hasil perkalian peluang masing-masing kejadian tersebut.

$$P(A \cap B) = P(A) \times P(B)$$

```{r echo=FALSE, out.width="50%"}
knitr::include_graphics("img/venn.png")
```

**Contoh:**

Peluang dadu keluar angka 4 pada lemparan pertama = P(A) = 1/6 Peluang
dadu keluar angka 4 pada lemparan kedua = P(B) = 1/6

$$P(A \cap B) = P(A) \times P(B) = \frac{1}{6} \times \frac{1}{6} = \frac{1}{36}$$
Jadi, peluang dadu keluar angka 4 pada lemparan pertama dan peluang
keluar angka 6 pada lemparan kedua = 1/36

-   **Dependent Event**: Peluang kejadian A dipengaruhi oleh peluang
    kejadian B (informasi tentang kejadian B). Contoh:

    -   Peluang banjir di Jakarta **jika diketahui** peluang hujan deras
        di Bogor
    -   Peluang kelas Algoritma online jika diketahui PSBB diterapkan di
        Jakarta
    -   Peluang pelanggan memberi rating rendah jika diketahui terdapat
        kata "Buruk" pada form feedback

Untuk menghitung peluangnya, kita menggunakan **Bayes Theorem**:

```{r echo=FALSE, out.width="50%"}
knitr::include_graphics("img/venn.gif")
```

P(sakit perut\|pagi makan sambel)

$$P(A|B) = \frac{P(B|A) P(A)}{P(B|A) P(A)\ +\  P(B|\neg A) P(\neg A)}$$
$P(A|B)$ = Peluang terjadi A jika diketahui B telah terjadi.

$P(B|\neg A)$ = Peluang tidak terjadi A jika diketahui B telah terjadi.

$P(A)$ = Peluang terjadi A

$P(\neg A)$ = Peluang tidak terjadi A

**Study Case: SPAM Classifier**

Sebagai seorang analis di salah satu provider jaringan di Indonesia,
fokus perusahaan saat ini tertuju pada banyaknya keluhan pelanggan
terkait sms SPAM. Perusahaan khawatir apabila hal ini tidak ditangani,
maka akan berpotensi menyebabkan terjadinya churn atau perpindahan
pelanggan ke provider kompetitor karena pelanggan terganggu dengan SPAM
tersebut.

Dikumpulkan sebanyak **1000 sms** yang diantaranya terapat **250 sms
SPAM**. Dengan melakukan text mining, diketahui bahwa dari seluruh sms
SPAM, terdapat **100 sms** dengan kata *Pinjaman* di dalamnya, dan hanya
terdapat **30 sms** yang memiliki kata *Pinjaman* namun bukan merupakan
SPAM.

Cika adalah seorang pelanggan dari provider tersebut. Bila Cika hari ini
menerima sms dengan kata *Pinjaman* di dalamnya, apakah sms tersebut
diperkirakan sebagai SPAM?

**Dicari:** Peluang sms termasuk **SPAM** jika diketahui terdapat kata
**Pinjaman** di dalamnya

**Diketahui:**

$$\begin {matrix}
 & Spam & \neg Spam \\
\hline
Pinjam & 100 & 30 \\
\neg Pinjam & 150 & 720
\end{matrix}$$

Jika kita menggunakan rumus kejadian independen untuk menghitung peluang
kejadian dependen, hasilnya akan terlalu optimistik:

$$ Peluang= \frac{250}{1000}\ \times \frac{130}{1000}\ = 0.0325 \ ?!? $$

Bila disimpulkan: sms yang terdapat kata *Pinjaman* di dalamnya bukan
merupakan SPAM -- peluang termasuk SPAM amat kecil (=.=)

Mari kita hitung menggunakan rumus kejadian dependen:

$$ P(Spam\ |\ Pinjam) = \frac{P(Spam)\ P(Pinjam\ |\ Spam)}{P(Spam)\ P(Pinjam\ |\ Spam) + P(\neg Spam)\ P(Pinjam\ |\neg Spam)} $$

\[NOTES: Pahami intuisinya saja tidak apa-apa\]

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("img/bayes.PNG")
```

Kita cari satu per satu peluang yang dibutuhkan:

-   Peluang sms adalah SPAM: $P(Spam) = \frac{250}{1000} = 0.25$
-   Peluang muncul kata Pinjaman di sms SPAM:
    $P(Pinjam\ |\ Spam) = \frac{100}{250} = 0.4$
-   Peluang sms bukan SPAM: $P(\neg Spam) = 750/1000 = 0.75$
-   Peluang muncul kata Pinjaman di sms yang bukan SPAM:
    $P(Pinjam\ |\neg Spam) = \frac{30}{750} = 0.04$

Kita masukkan angka peluang yang sudah kita hitung:

$$P(Spam\ |\ Pinjam) = \frac{0.25 \times 0.4}{0.25 \times 0.4\ +\ 0.75 \times 0.04} = 0.7692$$

```{r}
(0.25*0.4)/((0.25*0.4)+(0.75*0.04))
```

Peluang sms yang diterima Cika adalah SPAM sebesar 0.7692. Apabila kita
menentukan threshold treshold 0.5, kita bisa anggap bahwa sms tersebut
adalah SPAM.

‚ö†Ô∏è **NOTE:**: $P(Spam\ |\ Pinjaman) \neq P(Pinjaman\ |\ Spam)$

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("img/bayes2.PNG")
```

#### (OPTIONAL) Penurunan Rumus Bayes Theorem

1.  Diketahui rumus joint probability untuk dependent event. Ilustrasi
    lihat animasi diagram venn pada tab pertama.

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

2.  Event $A$ dan $B$ bisa ditukar, yang tadinya hipotesis menjadi
    evidence, begitupun sebaliknya:

$$P(B|A) = \frac{P(B \cap A)}{P(A)}$$

3.  $P(A \cap B) = P(B \cap A)$ karena sifatnya komutatif. Contoh:
    peluang muncul mata dadu 4 pada pelemparan pertama dan 6 pada
    pelemparan kedua **SAMA SAJA** dengan peluang muncul mata dadu 6
    pada pelemparan kedua dan 4 pada pelemparan pertama. Maka dari itu,
    kedua persamaan di atas dapat dibentuk menjadi:

$$P(A \cap B) = P(A|B) \times P(B)$$

$$P(A \cap B) = P(B|A) \times P(A)$$

4.  Kedua persamaan di atas mengacu pada nilai $P(A \cap B)$ yang sama
    sehingga:

$$P(A|B) \times P(B) = P(B|A) \times P(A)$$

5.  Terakhir, pindahkan $P(B)$ ke ruas kanan persamaan:

$$P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$$ ## Naive Bayes

Naive Bayes adalah model machine learning yang memanfaatkan Bayes
Theorem untuk melakukan klasifikasi. Hubungan antara prediktor dengan
target variabel dianggap saling dependen. Namun, Naive Bayes
mengasumsikan **tiap prediktor saling independen** (tidak berhubungan
satu sama lain) dan **memiliki bobot yang sama** untuk menghasilkan
prediksi--oleh karena itu dinamakan *"Naive"*. Hal ini untuk memudahkan
kalkulasi dan mengurangi beban komputasi.

Secara umum, Naive Bayes dapat dibedakan berdasarkan tipe data
predictornya:

1.  Bernoulli/Binomial NB: predictor kategorikal, 2 kelas (contoh case:
    Text Mining Spam Classifier)
2.  Multinomial NB: predictor kategorikal, multi kelas (contoh case:
    Market Analysis)
3.  Gaussian NB: predictor numerikal, asumsi distribusi normal (tidak
    dibahas pada course ini, silahkan mengacu pada [askalgo:
    C2](https://askalgo.netlify.app/#classification-2) pertanyaan
    nomor 4)

### **Study Case: Market Analysis** {.tabset}

**Business Question:** Sebagai seorang marketing analyst, kita ingin
meningkatkan penjualan dengan menargetkan customer dengan karakteristik
tertentu. Dari **400 customer** yang sudah kita prospek, kita dapatkan
informasi tentang gender, umur, dan kategori gaji serta apakah dia
membeli produk kita atau tidak.

-   Positive Class: membeli
-   Negative Class: tidak membeli

```{r, message=FALSE, warning=FALSE}
library(dplyr)

# read data Customer_Behaviour.csv
customer <- read.csv("data_input/Customer_Behaviour.csv", stringsAsFactors = T)
# cek data
head(customer)
```

Deskripsi data:

-   **Gender**: Jenis Kelamin (Male, Female)
-   **Age**: Range usia (\<30, 30-50, > 50)
-   **Estimated Salary**: Kategori Gaji Customer (Low, Medium, High)
-   **Purchased**: Apakah klien membeli produk kita atau tidak (Yes, No)

Apakah terdapat tipe data yang perlu diperbaiki? Tidak ada.

Klien kita berikutnya adalah Eca, seorang **wanita** berusia **24
tahun** dengan **gaji yang tinggi**. Berapa peluang Eca akan membeli
produk kita?

Kita formulakan:

$$P(Purchase\ | Gender = Female\ \cap\ Age<30\ \cap\ Salary = High) = P(Purchase| A\ \cap\ B\ \cap\ C)$$
Untuk mempersingkat notasi, kita simbolkan kejadian $A, B, C$
(prediktornya) sebagai:

-   $A$: Gender = Female
-   $B$: Age \< 30
-   $C$: Salary = High

Sehingga:

$$P(Purchase| A\ \cap\ B\ \cap\ C) = \frac{P(Purchase) \ P(A\ \cap\ B\ \cap\ C\ |\ Purchase)}{P(Purchase) \ P(A\ \cap\ B\ \cap\ C\ |\ Purchase)\ + \ P(\neg Purchase) \ P(A\ \cap\ B\ \cap\ C\ |\ \neg Purchase)}$$

**Asumsi Independensi Antar Variable Prediktor**

Naive Bayes mengasumsikan bahwa tiap prediktor saling independen atau
tidak berkaitan satu sama lain. Karena feature kita adalah
$A\ \cap\ B\ \cap\ C$, dapat diubah menjadi seperti berikut:

$$P(A\ \cap\ B\ \cap\ C\ |\ Purchase) = P(A\ |\ Purchase)\ P(B\ |\ Purchase)\ P(C\ |\ Purchase)\ $$

Sehingga:

$$= \frac{P(Purchase) \ P(A\ |\ Purchase)\ P(B\ |\ Purchase)\ P(C\ |\ Purchase)}{P(Purchase) \ P(A\ |\ Purchase)\ P(B\ |\ Purchase)\ P(C\ |\ Purchase)\ +\ P(\neg Purchase) \ P(A\ |\neg Purchase)\ P(B\ |\neg Purchase)\ P(C\ |\neg Purchase)}$$

#### Perhitungan Manual

Kita cari satu per satu peluang yang dibutuhkan:

-   Peluang seseorang membeli produk kita ($P(Purchase)$)

```{r}
# table frekuensi
prop.table(table(customer$Purchased))
```

Jika dihitung dengan cara manual :

```{r}
p_yes <- 143/nrow(customer)
p_no <- 257/nrow(customer)
```

-   Table Frekuensi Gender vs Purchase

```{r}
table("Gender" = customer$Gender, "Purchased" = customer$Purchased)
```

a.  Peluang seseorang adalah **wanita** ($A$) jika diketahui dia membeli
    produk kita ($P(A\ |\ Purchase)$)

```{r}
p_female_yes <- 77/(77+66)
```

b.  Peluang seseorang adalah **wanita** ($A$) jika diketahui dia tidak
    membeli produk kita ($P(A\ |\neg Purchase)$)

```{r}
p_female_no <- 127/(127+130)
```

-   Tabel Frekuensi Age vs Purchase

```{r}
table("Age" = customer$Age, "Purchased" = customer$Purchased)
```

a.  Peluang seseorang berumur \< 30 ($B$) jika diketahui dia membeli
    produk kita ($P(B\ |\ Purchase)$)

```{r}
p_30_yes <- 4/(4+45+94)
```

b.  Peluang seseorang berumur \< 30 ($B$) jika diketahui dia tidak
    membeli produk kita ($P(B\ |\neg Purchase)$)

```{r}
p_30_no <- 96/(96+4+157)
```

-   Tabel Frekuensi Salary vs Purchase

```{r}
table("Salary" = customer$Salary, "Purchased" = customer$Purchased)
```

a.  Peluang seseorang salary tinggi ($C$) jika diketahui dia membeli
    produk kita ($P(C\ |\ Purchase)$)

```{r}
p_high_yes <- 75/(75+39+29)
```

b.  Peluang seseorang salary tinggi ($C$) jika diketahui dia tidak
    membeli produk kita ($P(C\ |\neg Purchase)$)

```{r}
p_high_no <- 19/(19+56+182)
```

Kita kembali pada perhitungan peluang Eca purchase ketika diketahui
kondisi-kondisinya:

$$= \frac{P(Purchase) \ P(A\ |\ Purchase)\ P(B\ |\ Purchase)\ P(C\ |\ Purchase)}{P(Purchase) \ P(A\ |\ Purchase)\ P(B\ |\ Purchase)\ P(C\ |\ Purchase)\ +\ P(\neg Purchase) \ P(A\ |\neg Purchase)\ P(B\ |\neg Purchase)\ P(C\ |\neg Purchase)}$$

```{r}
(p_yes*p_female_yes*p_30_yes*p_high_yes)/((p_yes*p_female_yes*p_30_yes*p_high_yes)+
                                            p_no*p_female_no*p_30_no*p_high_no)

```

Kesimpulan: Karena probability Eca untuk membeli produk sama dengan
0.24, apabila threshold pengambilan keputusan kita untuk mengklasifikasi
Eca membeli atau tidak yaitu 0.5, artinya Eca tidak akan membeli.

Strategi apa yang harus dipersiapkan?

-   memberikan promo/cashback
-   menawarkan produk yang sesuai dengan personality Eca

#### `naiveBayes()` function

Pada R kita dapat menggunakan function `naiveBayes()` dari package
`e1071` untuk membuat model Naive Bayes.

Terdapat 2 cara dalam membuat model menggunakan function `naiveBayes()`
:

1.  Menggunakan argumen `naiveBayes(formula, data)`

-   `formula`: formula y\~x, dimana y: target variabel, x: prediktor
    variabel
-   `data`: data yang digunakan untuk target dan prediktor variabel

2.  Menggunakan argumen `naiveBayes(x, y)`

-   `x`: prediktor variabel dari data yang digunakan
-   `y`: target variabel dari data yang digunakan

```{r}
# install.packages("e1071")
library(e1071)

# Train Model
model_customer <- naiveBayes(formula = Purchased~., data = customer)
  
# check model
model_customer
```

Output dari `naiveBayes()`:

-   A-priori probability : nilai probability dari target variabel
    (proporsi target variabel)
-   Conditional probabilities : nilai probability untuk masing-masing
    variabel target terhadap variabel prediktor

```{r}
(0.3575*0.5384615*0.02797203*0.52447552)/((0.3575*0.5384615*0.02797203*0.52447552)+
                                            (0.6425*0.4941634*0.37354086*0.07392996))
```

Model Naive Bayes menyimpan informasi tentang nilai peluang dependen
antara target dengan setiap feature. Kita bisa periksa nilainya dengan
perhitungan manual.

-   `margin`: perhitungan proporsi berdasarkan 1 = baris, 2 = kolom

```{r}
prop.table(table(Gender = customer$Gender, Purchased = customer$Purchased),
           margin = 2) # penghitungan proporsi per baris
```

Prediksi wanita dengan gaji tinggi dan berusia di bawah 30 tahun.
`type = raw` mengembalikan nilai peluang kelasnya. `type = class`
mengembalikan nilai prediksi kelasnya, default function `predict()`
dalam model Naive Bayes.

Parameter `type`:

-   `type = "raw"` mengembalikan nilai peluang untuk masing-masing kelas
-   `type = "class"` mengembalikan label kelasnya (default threshold
    0.5)

```{r}

eca <- data.frame(Gender = "Female", 
                    Age = "< 30",
                    Salary = "High")
# predict probability
pred_eca_proba <- predict(object = model_customer, newdata = eca, type = "raw")
pred_eca_proba

# predict class
pred_eca_class <- predict(object = model_customer, newdata = eca, type = "class")
pred_eca_class

```

üí° **IMPORTANT POINTS**

-   Naive Bayes mengklasifikasi berdasarkan peluang dependen antara
    prediktor dengan target variabel

-   Naive Bayes mengasumsikan:

    -   setiap prediktor memiliki bobot yang sama (sama-sama dianggap
        penting)
    -   antar prediktor saling independen

-   Kelebihan:

    -   waktu training model lebih cepat dikarenakan asumsi setiap
        prediktor memiliki bobot yang sama
    -   model naive bayes biasanya dijadikan sebagai *base classifier*
        atau benchmark model untuk acuan pembanding model yang lebih
        kompleks

## Laplace Smoothing

Pada beberapa kasus, dapat terjadi data scarcity dimana *salah satu
prediktor memiliki nilai yang tidak ada sama sekali di salah satu
kelas*. Misalnya kita modifikasi dari data Customer Behaviour
sebelumnya, pada golongan orang-orang yang *Purchase* sama sekali tidak
ada yang berjenis kelamin wanita sehingga table frekuensinya menjadi
sebagai berikut:

```{r echo=FALSE}
matrix(c(127, 130, 0, 143), nrow = 2, dimnames = list(c("Female", "Male"), c("No Purchase", "Purchase")))
```

Apabila tabel frekuensi untuk prediktor lainnya tetap sama, maka peluang
Eca sebagai **wanita** dengan **usia \< 30** dengan **gaji tinggi**
untuk membeli produk:

$$P(Purchase\ |\ Age <30 \ \cap\ Gender = Female\ \cap\ Salary = High ) \\
= \frac{\frac{143}{143+257}\ \frac{4}{4+45+94}\ \frac{0}{0+143}\ \frac{75}{75+39+29}}
{\frac{143}{143+257}\ \frac{4}{4+45+94}\ \frac{0}{0+143}\ \frac{75}{75+39+29} + \frac{257}{143+257}\ \frac{96}{96+4+157}\ \frac{127}{127+130}\ \frac{19}{19+56+182}} = 0$$

Karena terdapat pembilang yang memiliki nilai 0, maka peluangnya akan
langsung menjadi **0**.

Ini adalah karakteristik kedua dari Naive Bayes: **Skewness Due To
Scarcity**. Ketika terdapat *suatu prediktor yang frekuensi nilainya 0
untuk salah satu kelas* (pada kasus ini `0 Female` untuk
`Purchase = Yes`), maka *model secara otomatis memprediksi bahwa
peluangya adalah 0 untuk kondisi tersebut, tanpa memperdulikan nilai
dari prediktor yang lainnya*. Alhasil, model akan secara otomatis
memprediksi peluang = 0 (tidak beli) bila customer adalah wanita, dan
otomatis memprediksi peluang = 1 (beli) bila customer adalah pria. Model
menjadi bias atau kurang akurat dalam melakukan prediksi.

Apa yang harus kita lakukan jika menemukan fenomena tersebut?

-   **Strategi 1: Menghilangkan prediktor yang bermasalah**

Prediktor yang bermasalah bisa dihilangkan atau tidak digunakan untuk
pembuatan model. Namun, cara ini mungkin kurang tepat karena bisa saja
feature tersebut memang punya pengaruh besar terhadap hasil prediksi,
sehingga ketika dihilangkan maka hasil prediksi justru kurang baik.

-   **Strategi 2: Menggunakan Laplace Smoothing**

Solusi alternatifnya adalah Laplace Smoothing yaitu *menambahkan
frekuensi dari setiap prediktor sebanyak angka tertentu (biasanya 1)*,
sehingga tidak ada prediktor yang memiliki nilai 0 dan harus dibuang.

**Tabel Frekuensi Gender**

$$\begin {matrix}
 & Purchase & No\ Purchase \\
\hline
Female & 127+1 & 0+1 \\
Male & 130+1 & 143+1
\end{matrix}$$

**Tabel Frekuensi Age**

$$\begin {matrix}
 & Purchase & No\ Purchase \\
\hline
<30 & 96+1 & 4+1 \\
>50 & 4+1 & 45+1 \\
30-50 & 157+1 & 94+1
\end{matrix}$$ **Tabel Frekuensi Salary**

$$\begin {matrix}
 & Purchase & No\ Purchase \\
\hline
Salary\ High & 19+1 & 75+1 \\
Salary\ Low & 56+1 & 39+1 \\
Salary\ Medium & 182+1 & 29+1
\end{matrix}$$

Jika kita kembali menghitung peluang Eca untuk membeli produk kita dari
persamaan sebelumnya:

$$P(Purchase\ |\ Age <30 \ \cap\ Gender = Female\ \cap\ Salary = High ) \\
= \frac{\frac{144}{144+258}\ \frac{5}{5+46+95}\ \frac{1}{1+144}\ \frac{76}{76+40+30}}
{\frac{144}{144+258}\ \frac{5}{5+46+95}\ \frac{1}{1+144}\ \frac{76}{76+40+30} + \frac{258}{144+258}\ \frac{97}{97+5+158}\ \frac{128}{128+131}\ \frac{20}{20+57+183}} = 0.0048$$

```{r}
((144/(144+258))*(5/(5+46+95))*(1/(1+144))*(76/(76+40+30)))/
  ((144/(144+258))*(5/(5+46+95))*(1/(1+144))*(76/(76+40+30)) +
     (258/(144+258))*(97/(97+5+158))*(128/(128+131))*(20/(20+57+183)))
```

```{r}
# pembilang <- ((144/(144+258))*(5/(5+46+95))*(1/(1+144))*(76/(76+40+30)))
# penyebut <- ((144/(144+258))*(5/(5+46+95))*(1/(1+144))*(76/(76+40+30)) +
#      (258/(144+258))*(97/(97+5+158))*(128/(128+131))*(20/(20+57+183)))
# pembilang/penyebut
```

Meskipun peluangnya masih sangat kecil, tetapi setidaknya dengan metode
Laplace Smoothing bisa memastikan model tidak terlalu ekstrim dalam
menentukan nilai target serta tetap dapat mempertimbangkan nilai peluang
dari prediktor lainnya.

Menggunakan Laplace Smoothing pada model sebelumnya cukup menambahkan
argumen `laplace = 1`, artinya kita menambahkan 1 untuk setiap frekuensi
nilai prediktor yang ada.

```{r eval=FALSE}
# pembuatan mdoel naiveBayes menggunakan laplace
model_customer2 <- naiveBayes(formula = Purchased~., data = customer, laplace = 1)

```

üí° **IMPORTANT POINTS**

-   Kekurangan Naive Bayes: **skewness due to data scarcity** yaitu
    timbul bias dalam memprediksi dikarenakan ada salah satu kejadian
    yang tidak muncul dalam data kita

-   **Laplace Smoothing** merupakan menambahkan nilai tertentu pada
    setiap prediktor dan kategorinya supaya tidak ada kondisi
    probability = 0 untuk melakukan *tunning model* dalam mengatasi
    kondisi skewness due to data scarcity

## üß† DIVE DEEPER: Party Affiliation

Kita akan coba menganalisis data dari **United States Congressional
Voting tahun 1984**. Data berisi informasi tentang hasil voting atau
dukungan dari masing-masing anggota kongres dangan affiliasi partai yang
berbeda (republican/democrat) terhadap berbagai isu atau kebijakan
negara.

**Business Question:** Ingin dilakukan klasifikasi apakah seseorang
cenderung berafiliasi dengan partai republican/democrat.

-   Positive Class: either
-   Negative Class: either

### Read Data

Read data `votes.txt` dengan `header = F` agar observasi pertama tidak
dianggap sebagai header atau nama kolom.

```{r}
# read data
party <- read.csv("data_input/votes.txt", header = F, stringsAsFactors = T)
# cek data
head(party)
```

### Data Wrangling

Ubah nama kolom menggunakan ketentuan berikut:

```{r}
names(party) <- c("party",
                  "hcapped_infants",
                  "watercost_sharing",
                  "adoption_budget_reso",
                  "physfee_freeze",
                  "elsalvador_aid",
                  "religious_grps",
                  "antisatellite_ban",
                  "nicaraguan_contras",
                  "mxmissile",
                  "immigration",
                  "synfuels_cutback",
                  "education_funding",
                  "superfundright_sue",
                  "crime",
                  "dutyfree_exps",
                  "expadmin_southafr"
                  )

# cek data
head(party)
```

Keterangan:

-   y = yes (setuju)
-   n = no (tidak setuju)
-   ? = tidak vote

Adakah tipe data yang perlu diperbaiki? Tidak ada yang perlu diperbaiki

### Cross-Validation

Split datanya menjadi data training dan data testing, dengan 75% data
akan digunakan sebagai data training.

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)
intrain <- sample(nrow(party), nrow(party)*0.75)
party_train <- party[intrain, ]
party_test <- party[-intrain, ]
```

Cek proporsi kelas target:

```{r}
prop.table(table(party_train$party))
```

### Model Fitting

Dalam model fitting, kita akan mencoba memodelkan menggunakan Naive
Bayes tanpa melakukan tunning model.

```{r}
model_party <- naiveBayes(formula = party ~ ., data = party_train, laplace = 1)

```

Model Interpretation

```{r}
model_party
```

Kita bisa menginterpretasikan masing-masing prediktor dari hasil
perhitungan peluang dependennya dengan variabel target. Contoh:

-   peluang anggota kongres berafiliasi dengan partai republican saat ia
    tidak menyetujui kebijakan `hcapped_infants` adalah 0.80597015
-   peluang anggota kongres berafiliasi dengan partai demokrat saat ia
    menyetujui kebijakan `adoption_budget_reso` adalah 0.86868687

### Model Prediction

Prediksi **kelas** dari data test dengan function `predict()`, masukan
ke dalam objek `party_predClass`:

```{r}
# predict
party_predClass <- predict(object = model_party, newdata = party_test, type = "class")
head(party_predClass)
```

### Model Evaluation

Evaluasi model dengan confusion matrix:

```{r}
library(caret)
confusionMatrix(data = party_predClass, reference = party_test$party, positive = "democrat")
```

Metrics mana yang ingin diunggulkan?

> Memilih accuracy karena pada kondisi ini, kita tidak mengunggulkan
> dari salah satu partai apapun, sehingga kita ingin model yang kita
> buat dapat memprediksi kedua kelas dengan baik.

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("img/accuracy_limitation.PNG")
```

Ketika kita menggunakan metric accuracy, hal ini akan sangat tepat
dipilih ketika data yang kita miliki balance, karena accuracy akan
memperhitungkan setiap TP dan TN yang berhasil ditebak oleh model.
Namun, ketika data kita imbalance, dan kita ingin melihat seberapa baik
model yang kita miliki dalam membedakan kelas Positive dan Negative,
kita bisa menggunakan metric ROC(Receiver-Operating Curve) dan AUC(Area
Under Curve).

# ROC and AUC

**ROC** dan **AUC** adalah tools evaluasi lain setelah Confusion Matrix,
untuk mengevaluasi kebaikan model dalam mengklasifikasi kedua kelas
(kelas positif & kelas negatif) dengan baik.

## Receiver-Operating Curve (ROC)

**Receiver-Operating Curve (ROC)** memplotkan hubungan antara True
Positive Rate (Sensitivity atau Recall) dengan False Positive Rate
(1-Specificity). Model yang baik idealnya memiliki **True Positive Rate
yang tinggi dan False Positive Rate yang rendah**.

**Intuisi Imbalance Class**

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("img/roc-animation.gif")
```

Mari kita buat kurva ROC dari model `model_party`:

```{r}
# ambil hasil prediksi probability
pred_partyProb <- predict(model_party, newdata = party_test, type = "raw")
```

Siapkan data frame untuk ROC (sebenarnya opsional, namun akan
mempermudah). Kita asumsi kelas positifnya adalah "democrat".

```{r}
# siapkan data frame untuk ROC
df_roc_party <- data.frame(prob = pred_partyProb[,1], # subset probability prediction untuk democrat
                           label = as.numeric(party_test$party == "democrat")) # data actual label untuk kelas positive democrat, 0 = republican, 1 = democrat
df_roc_party
```

Kita dapat buat ROC dengan menyiapkan objek `prediction()`:

```{r message=FALSE}
# install.packages("ROCR", dependencies = T)
library(ROCR)

# buat objek prediction
prediction_roc_party <- prediction(predictions = df_roc_party$prob, # mengambil kolom prediksi prob
                                  labels = df_roc_party$label) # mengambil label dari kelas target

# buat plot dari objek prediction
plot(performance(prediction.obj = prediction_roc_party, 
                 measure = "tpr", # nilai true positive rate (y axis)
                 x.measure = "fpr")) # nilai false positive rate (x axis)
```

-   TPR: dari seluruh anggota yang actualnya Democrat, berapa persen
    model kita mengklasifikasi dengan benar (tetap diprediksi sebagai
    Democrat)?

-   FPR: dari seluruh anggota yang actualnya Republik, berapa persen
    model kita salah mengklasifikasi (diprediksi jadi Democrat)?

-   Model yang buruk adalah yang menghasilkan garis diagonal pada plot
    -> model hanya baik memprediksi salah satu kelas.

## Area Under ROC Curve (AUC)

AUC menunjukkan luas area di bawah kurva ROC. **Semakin mendekati 1**,
semakin bagus performa modelnya. Untuk mendapatkan nilai AUC, tulis
`auc` pada parameter `measure` dari `performance()` dan ambil nilai
`y.values`.

```{r}
auc_party <- performance(prediction.obj = prediction_roc_party, measure = "auc")
auc_party@y.values
```

Nilai AUC dari model Naive Bayes mendekati 1, artinya model kita mampu
mengklasifikasikan kelas positif dan kelas negatif dengan baik.

üí° **Important Notes**:

ROC & AUC sebagai tools tambahan untuk menentukan kebaikan model
mengklasifiksaikan kelas positif maupun kelas negatif:

-   diinginkan kurva yang seperti huruf L terbalik atau membentuk garis
    tegak lurus dimana nilai TPR tinggi dan FPR rendah
-   diinginkan AUC yang mendekati angka 1, tidak diinginkan AUC yang
    mendekati 0.5.

# üìù Summary Day 1

-   Naive bayes merupakan metode klasifikasi dimana menggunakan konsep
    Bayes Theorem.

-   Naive bayes menganggap semua prediktor sama penting (semua prediktor
    independen)

-   Komputasi modelnya lebih ringan karena melakukan perhitungan
    probability setiap kejadian

-   kekurangan naive bayes sensitive terhadap ketidak munculan kondisi
    pada data (*skewness due to data scracity*)

-   Cara mengatasi kekurangan naive bayes :

    -   bisa menambahkan 1 angka pada parameter `laplace = 1`
    -   bisa menghilangkan variabel prediktor yang terindikasi tidak
        terdapat kejadian yang ada

-   ROC dan AUC sebagai alat evaluasi model:

    -   ROC adalah kurva yang menggambarkan TPR vs FPR untuk setiap
        threshold, sedangkan AUC adalah luas area di bawah kurva ROC.
    -   Semakin cembung kurva ROC (artinya, AUC mendekati nilai satu),
        semakin baik model dalam memisahkan kelas positif dan negatif.
    -   Model yang **baik** memiliki nilai **AUC mendekati 1**,
        sedangkan model yang **tidak baik** memiliki nilai **AUC
        mendekati 0.5**.

<center>

**---\[End of Day 1\]---**

</center>

<center>

**---\[Start of Day2\]---**

<center>

# üìë Text Mining

Text Mining adalah salah satu metode analisis data yang fokus utamanya
adalah mencari informasi dan pola-pola dari data yang **tidak
terstruktur, yaitu data teks**. Data teks disebut tidak terstruktur
karena:

-   Satu kalimat terdiri dari beberapa kata yang jumlahnya berbeda-beda
    tiap kalimat.
-   Adanya *typo* (salah ketik), penyingkatan kata (you menjadi u),
    ataupun simbol-simbol tidak bermakna sehingga perlu dilakukan
    cleansing.
-   Adanya perbedaan bahasa yang digunakan sehingga perlu mencari kosa
    kata yang cocok.

Berikut ilustrasi workflow dari text mining:

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/text_mining.png")
```

## Study Case: Spam Classifier

**Business Question:** Berdasarkan kata-kata pada SMS, kita ingin
melakukan klasifikasi apakah suatu SMS termasuk spam atau bukan (ham)?

-   Kelas positif: spam
-   Kelas negatif: ham (not spam)

## Read Data

```{r}
sms <- read.csv("data_input/spam.csv",
                    stringsAsFactors = FALSE,
                    encoding = "UTF-8")
str(sms)
```

## Data Wrangling

**Dive Deeper**

*Est. time 5 minutes*

Adakah kolom yang tidak berguna? Adakah kolom yang tipe datanya perlu
diperbaiki?

1.  Hapus 3 kolom terakhir yang tidak memiliki nilai dan informasi
2.  Gantilah nama kolom pertama menjadi "label" dan kolom kedua menjadi
    "text". Jangan lupa simpan perubahannya pada object dataframe `sms`
    kembali.\
3.  Berapa banyak proporsi sms "spam" pada data `sms`?
4.  Apakah kolom `label` sudah dalam bentuk factor? Jika belum, ubahlah
    dalam tipe data factor yang menyimpan 2 levels nilai kategori.

```{r}
# your code here
sms <- sms %>% 
  select(v1, v2)

colnames(sms) <- c("label", "text")

sms$label <- as.factor(sms$label)

prop.table(table(sms$label))
```

## Exploratory Data Analysis (EDA)

Silahkan ambil 5 sample text yang termasuk spam, kemudian amati
kata-kata apa saja yang dapat menjadi indikator (prediktor) bahwa suatu
text adalah spam?

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)
# your code here
sms %>% 
  filter(label == "spam") %>% 
  head(5)
```

Kata yang berpotensial mengindikasi bahwa suatu text adalah spam: free,
prize, claim, win, phone, number.

## Data Preprocessing

Data text (raw) perlu dibersihkan terlebih dahulu (text cleansing)
sebelum pembuatan model. Text dapat diubah menjadi format `corpus`
kemudian kita bersihkan.

### Text to Corpus

**Corpus** adalah kumpulan dari dokumen. Pada kasus ini, satu dokumen
ekuivalen dengan satu observasi SMS. Di dalam satu SMS bisa terdapat
satu atau lebih kalimat.

Salah satu package yang bisa kita gunakan untuk text mining adalah `tm`.
Pengubahan dari vector text menjadi corpus bisa dilakukan menggunakan
function `VCorpus()`

```{r}
# install.packages("tm")
library(tm)

# ubah format menjadi corpus
sms_corpus <- VCorpus(VectorSource(sms$text))

sms_corpus[[1]]$content
```

Untuk selanjutnya mari kita inspect content SMS ke-9:

```{r}
# cek content ke-9
sms_corpus[[9]]$content
```

### Text Cleansing

Dalam text cleansing ini, kita akan coba lakukan beberapa proses
cleansing, sbb:

-   `tolower` : Mengubah semua huruf kapital menjadi huruf kecil

```{r}
dat <- c("Algoritma ALGORITMA ALGOritma")
dat <- dat %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(content_transformer(tolower))

dat[[1]]$content
```

```{r}
# case-folding: mengubah semua text menjadi lowercase

sms_corpus_clean <- sms_corpus %>% 
  tm_map(content_transformer(tolower))

sms_corpus_clean[[9]]$content
```

-   `removeNumbers()` : Menghapus angka

```{r}
dat_num <- "324algoritma81919" %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(removeNumbers)
dat_num[[1]]$content
```

```{r}
# remove numbers: menghapus angka
sms_corpus_clean <- sms_corpus_clean %>% 
  tm_map(removeNumbers)

sms_corpus_clean[[9]]$content
```

-   `removeWords()` : Menghilangkan kata-kata umum yang kurang bermakna
    (stopwords)

```{r}
dat_stop <- "Return various kinds of stopwords with support for different languages" %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(removeWords, stopwords("english"))
dat_stop[[1]]$content
```

```{r}
# remove stopwords: menghapus kata yang sering muncul di corpus dan biasanya tidak meaningful

```

**Notes**: apabila ingin melakukan penghapusan stopwords pada teks
bahasa Indonesia dapat menggunakan package
[`katadasaR`](https://github.com/nurandi/katadasaR)

-   `removePunctuation()` : Menghilangkan tanda baca

Untuk menghilangkan semua tanda baca (punctuation), gunakan
`removePunctuation()`. Tanda baca yang dihilangkan: ! ' \# S % & ' ( )
\* + , - . / : ; \< = \> ? \@ \[ / \] \^ \_ { \| } \~

```{r}
dat_pc <- ("algo??\\ritma!!@$&^*+") %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(removePunctuation)

dat_pc[[1]]$content
```

Untuk menghilangkan/me-replace karakter tertentu, kita dapat membuat
function DIY.

```{r}
# function DIY
transformer <- content_transformer(FUN = function(x, pattern){
 gsub(x = x, 
      pattern = pattern, 
      replacement = " ") 
})
```

```{r}
# replace ".", "/", "@", "-" with a white space

# remove all punctuation, but without white space (dari package tm)


# cek content ke-9
```

-   `stemDocument()` : Menghapus kata-kata imbuhan yang kurang bermakna
    atau pemotongan kata menjadi kata dasarnya. Misalnya *walking*,
    *walked*, *walks* menjadi *walk*.

**mengubah kata dasar**

```{r}
stemDocument(c("do", "doing", "kiss", "kisses", "kissing", "playing"))
```

`stemDocument()` akan menghapus imbuhan kata "ing", "e", "ed", "er",
"s", "es", dll

Mengapa kata imbuhan dihapuskan? Karena yang dibutuhkan dalam text
mining ini adalah mendapatkan beberapa kata-kata yang muncul atau tidak
muncul dalam suatu sekumpulan text dimana kata-kata tersebut akan
digunakan sebagai prediktor untuk memprediksi apakah sms masuk kedalam
"spam" atau "ham".

```{r}
tes <- "updating update updated" %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(stemDocument)

tes[[1]]$content
```

```{r}
# lakukan instalasi package: SnowballC
# stemming


# cek content ke-9 

```

**Note:** Stemming hanya melakukan pemotongan kata, apabila ingin lebih
akurat kita dapat menggunakan teknik lemmatizing (lookup kata sebenarnya
sesuai kaidah bahasa, contoh: receiving -> receive). Namun trade-offnya,
komputasi lemmatizing lebih lama dibandingkan stemming. Silahkan mengacu
pada referensi untuk penggunaan lemmatizing.

-   `stripWhitespace()` : Menghapus spasi berlebih (white space)

Terakhir, kita melakukan remove space yang berlebihan "\_\_*" karena
proses tokenizing dan pembuatan document-term-matrix akan memotong kata
perkata dengan separator spasi "*",

```{r}
dat_space <- "algo data      science" %>% 
  VectorSource() %>% 
  VCorpus() %>% 
  tm_map(stripWhitespace)
dat_space[[1]]$content
```

```{r}
# remove white space
sms_corpus_clean <- sms_corpus_clean %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords("english")) %>% 
  tm_map(stemDocument) %>% 
  tm_map(stripWhitespace)

sms_corpus_clean[[9]]$content
```

Summary singkat, secara umum tahapan yang sering dilakukan untuk text
cleansing adalah:

-   Case-folding
-   Remove numbers
-   Remove stop-words
-   Remove punctuation
-   Stemming
-   Remove white space

### Document-Term Matrix (DTM)

Sampai di tahap ini, data kita masih berupa text. Pertanyaannya
bagaimana cara model kita belajar apabila prediktornya masih berupa
text?

Kita perlu melakukan transformasi data text menjadi **Document-Term
Matrix (DTM)** melalu proses tokenization. **Tokenization** adalah
proses memecah satu kalimat menjadi beberapa *term* (bisa berupa 1 kata,
pasangan kata, dll). Dalam DTM, 1 kata akan menjadi 1 prediktor dengan
nilai berupa frekuensi kemunculan kata tersebut dalam sebuah dokumen.

```{r}
# ubah menjadi DTM
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
inspect(sms_dtm)
```

Istilah:

-   documents: SMS
-   terms: kata yang unique di seluruh SMS kita
-   non-sparse: nilai yang bukan 0 pada matrix
-   sparse: nilai yang 0 pada matrix

Mari kita amati SMS ke 1085 yang sudah di-cleansing, kemudian dapat kita
konfirmasi bahwa kata "will" muncul sebanyak 12 kali:

```{r}
sms_corpus_clean[[1085]]$content
```

## Cross-Validation

Splitting data menjadi `sms_train` dan `sms_test` dengan perbandingan
75-25.

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)
# melakukan splitting berdasarkan sms_dtm
intrain <- sample(nrow(sms_dtm), nrow(sms_dtm) * 0.75)
# train-test splitting
sms_train <- sms_dtm[intrain, ]
sms_test <- sms_dtm[-intrain, ]


```

Siapkan juga label untuk targetnya:

```{r}
# label untuk train dan test, tersimpan pada dataframe sms
label_train <- sms[intrain, "label"]
label_test <- sms[-intrain, "label"]
```

Cek proporsi kelas target pada `label_train` dan `label_test`:

```{r}
prop.table(table(label_train))
prop.table(table(label_test))
```

Note Penting: Pada kasus ini memang SMS kita cukup imbalance. Namun pada
tahap ini kita ingin melihat perfoma model tanpa dilakukannya balancing.

## Further Data Preprocessing

### Remove Infrequent Words

Cek dimensi `sms_train` yang akan digunakan untuk pembuatan model:

```{r}
dim(sms_train)
```

Predictor kita sangat banyak, mencapai 6183. Mari kita kurangi noise
pada data kita dengan mengambil kata-kata yang cukup sering muncul,
misalnya minimal 20 kali dalam keseluruhan SMS. Gunakan function
`findFreqTerms()`:

```{r}
# sms_train dari sms_dtm
sms_freq <- findFreqTerms(x = sms_train, lowfreq = 20)
```

**Note:** Penentuan `lowfreq = 20` tidak mutlak dan dapat diubah-ubah
untuk *feature selection*. Perlu diketahui: Semakin besar `lowfreq`,
semakin sedikit terms yang kita gunakan sebagai feature/predictor.

Mari subset data `sms_train` hanya untuk kata-kata yang muncul di
`sms_freq`:

```{r}
sms_train <- sms_train[, sms_freq]
dim(sms_train)
```

### Bernoulli Converter

Nilai pada matrix `sms_train` masih berupa frekuensi. Untuk perhitungan
peluang, frekuensi akan diubah menjadi hanya kondisi muncul (1) atau
tidak (0). Salah satu caranya dengan menggunakan **Bernoulli
Converter**.

-   Jika frekuensi > 0, maka bernilai 1 (muncul)
-   Jika frekuensi == 0, maka bernilai 0 (tidak muncul)

```{r}
bernoulli_conv <- function(x){
  # parameter ifelse: kondisi, TRUE, FALSE
  x <- as.factor(ifelse(x > 0, 1, 0)) 
  return(x)
}

# testing fungsi
bernoulli_conv(c(3,0,0,1,4,0))
```

Selanjutnya, terapkan `bernoulli_conv` ke `sms_train` dan `sms_test`:

```{r}
sms_train_bn <- apply(sms_train, MARGIN = 2, FUN = bernoulli_conv)
sms_test_bn <- apply(sms_test, MARGIN = 2, FUN = bernoulli_conv)
```

-   MARGIN = 1 -> mengaplikasikan FUN by baris
-   MARGIN = 2 -> mengaplikasikan FUN by kolom, karena kita ingin tetap
    matrix berupa DocumentTermMatrix

Mari cek hasilnya:

```{r}
sms_train_bn[4:10, 50:55]
```

## Model Fitting

Membuat model Naive Bayes dengan `sms_train_bn` yang sudah siap:

```{r}
# train
model_spam <- naiveBayes(x = sms_train_bn, y = label_train, laplace = 1)
```

## Model Prediction

Prediksi kelas target pada `sms_test_bn`. Simpan ke objek
`sms_pred_class`, akan digunakan untuk mengevaluasi dengan confusion
matrix.

```{r}
# predict
y_pred <- predict(object = model_spam, newdata = sms_test_bn, type = "class")
```

## Model Evaluation

### Confusion Matrix

Evaluasi model `model_spam` menggunakan confusion matrix dan
metric-metric yang ada:

```{r}
confusionMatrix(data = y_pred, reference = label_test, positive = "spam")
```

Kasus mana yang ingin kita minimalisir:

\- False Negative: sms kita sebenarnya spam, tapi model prediksi sebagai
ham (akibatnya sms spam masuk inbox kita)

\- False Positive: sms kita sebenarnya ham, tapi diprediksi sebagai spam
(akibatnya sms ham masuk ke folder spam kita)

Minimalisir: False Positive

1.  Metrics mana yang kita perhatikan? Sertakan alasannya:

    Precision, karena kita ingin sebaik mungkin menentukan apakah sms
    termasuk ke spam atau tidak dan sms yang penting tidak masuk kedalam
    spam.

2.  Menurut metrics tersebut, apakah performa model kita sudah cukup
    baik?

### üß† (Dive Deeper) ROC and AUC

Pertama, siapkan hasil prediksi berupa probability dari `sms_test_bn`,
simpan ke objek dengan nama `sms_pred_prob`:

```{r}

```

Siapkan data ROC untuk mempermudah kita, simpan ke objek dengan nama
`data_sms_roc`:

```{r}

```

Buat kurva ROC. Simpan hasil object `prediction()` dengan nama
`sms_roc`:

```{r}

```

Hitung nilai AUC:

```{r}

```

Kesimpulan:

**Summary text mining**

1.  Naive Bayes dapat digunakan untuk kasus klasifikasi dari data text
2.  Workflow pada text mining/analysis adalah:

-   ...

-   ...

-   ...

-   data pre-processing:

    -   ...
    -   ...
    -   ...

-   ...

-   further data pre-processing:

    -   ...
    -   ...

-   ...

-   ...

-   ...

Pilihan:

-   pembuatan model
-   EDA
-   data wrangling
-   corpus cleansing
-   text -> corpus
-   read data\
-   corpus -> dtm
-   cross-validation
-   beroulli converter
-   feature selection dengan `findFreqTerms()`
-   prediction
-   model evaluation

# Decision Tree

Decision Tree merupakan *tree-based model* yang cukup sederhana dengan
performa yang *robust/powerful* untuk prediksi. Decision Tree
menghasilkan visualisasi berupa **pohon keputusan** yang *dapat
diinterpretasi* dengan mudah.

Karakter tambahan Decision Tree:

-   Variable predictor diasumsikan saling dependent, sehingga dapat
    mengatasi multicollinearity.
-   Dapat mengatasi nilai predictor numerik yang berupa outlier.

**Note**: Decision Tree tidak hanya terbatas pada kasus Classification,
namun dapat digunakan pada kasus Regression. Pada course ini fokus kita
ke kasus Classification karena idenya sama.

## Struktur

Mari memahami bagaimana struktur dari Decision Tree dan istilah yang
sering digunakan. Berikut contoh untuk menentukan apakah weekend ini
kita akan *beraktifitas keluar atau tidak*:

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/strucutre.PNG")
```

-   **Root Node**: Percabangan pertama dalam menentukan nilai target,
    biasa disebut sebagai predictor utama.
-   **Interior Node**: Percabangan selanjutnya yang menggunakan
    predictor lain apabila root node tidak cukup dalam menentukan
    target.
-   **Terminal/Leaf Node**: Keputusan akhir berupa nilai target yang
    diprediksi.

Pertanyaan selanjutnya, bagaimana Decision Tree memilih predictor pada
setiap percabangannya?

-   Secara intuitif, Decision Tree memilih predictor yang sebisa mungkin
    meng-homogen-kan target variable pada leaf node nya. Contoh:
    Observasi 100% Yes ketika Outlook (cuaca) bernilai Overcast
    (berawan).
-   Tingkat kehomogenan ini dapat dikuantifikasi menggunakan **Entropy**
    dan **Information Gain**.

## Entropy and Information Gain {.tabset}

### Intuition

Misalkan saya mencatat perilaku saya dalam menentukan **apakah saya
pergi makan ke restoran** selama 12 hari terakhir. Berikut saya
menggunakan Decision Tree untuk menemukan pola perilaku sehingga dapat
memprediksi keputusan kedepannya:

```{r}
dine <- read.csv("data_input2/dineout.csv",
                 sep = ";",
                 stringsAsFactors = T)
dine
```

-   **Budget**: Budget yang saya miliki (High atau Low)
-   **Distance**: Jarak restoran dari rumah saya (Far atau Near)
-   **Friend**: Apakah ada teman yang menemani? (Available atau Absent)
-   **Dine.Out**: Keputusan saya apakah makan di restoran atau tidak
    (Yes atau No)

Tujuan Decision Tree yaitu memisahkan data menjadi kelompok-kelompok
kecil berdasarkan variable tertentu sehingga dihasilkan **data yang
homogen** atau homogenitas tinggi. Ukuran homogenitas dapat
dikuantifikasi dengan nilai **entropy**.

**Entropy** adalah ukuran ketidakteraturan (measure of disorder) dari
sebuah kelompok data.

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("img/entropy.PNG")
```

-   **Entropy = 1**: tidak ada kelas yang dominan (proporsi seimbang,
    50:50)
-   **Entropy = 0**: salah satu kelas sangat dominan (proporsi 100:0)

> Kelompok data yang diharapkan setelah dilakukan percabangan adalah
> kelompok yang memiliki entropy rendah.

Untuk memilih prediktor mana yang menjadi root node, dihitunglah
**perubahan entropy** yaitu selisih antara entropy sebelum dan sesudah
dilakukan percabangan menggunakan variable predictor.

Predictor yang dipilih adalah predictor yang menghasilkan **penurunan
entropy paling besar**, berarti membuat data setelah pemisahan semakin
homogen. Perubahan entropy inilah yang disebut **Information Gain**.

> Prediktor yang dipilih pada setiap percabangan adalah predictor yang
> menghasilkan information gain terbesar.

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/dineout.PNG")
```

> Ilustrasi untuk Splitting Decision Tree pada kasus di atas dapat
> dilihat secara detail pada [Google
> Sheets](https://docs.google.com/spreadsheets/d/1WWeZdZ2vDIK9c_iQ2yQuEjgWOW6Fm8gpO3qKt0D_Zx8/edit#gid=0)

Visualisasi Decision Tree:

```{r echo = FALSE, fig.width = 8}
library(partykit)

```

‚ö†Ô∏è **Peringatan:** Decision tree di atas hanyalah sebagai ilustrasi yang
menggunakan data dummy dengan jumlah observasi masih sangat sedikit.

### (OPTIONAL) Perhitungan Manual

Rumus Entropy:

$$Entropy = \Sigma_{i=1}^c -p_i \ log_2 \ p_i$$

Apabila untuk binary classification (target dengan dua kelas), maka
$c = 2$ dapat dijabarkan menjadi:

$$Entropy = -\ p_1 \ log_2 \ p_1 -p_2 \ log_2 \ p_2$$

-   $p_1$: proporsi kelas pertama, misalkan kelas positif
    `Dine.Out = Yes`
-   $p_2$: proporsi kelas kedua, misalkan kelas negatif `Dine.Out = No`

Pertama-tama hitung entropy dari variable target `Dine.Out` sebelum
dilakukan splitting:

```{r}
table(dine$Dine.Out)
```

```{r}
p1 <- 4/(8+4)
p2 <- 8/(8+4)
entropy_awal <- - p1*log2(p1) - p2*log2(p2)
entropy_awal
```

Selanjutnya, kita coba splitting data menggunakan predictor satu per
satu predictor secara iteratif, kemudian pilih satu predictor yang mampu
menghasilkan **penurunan entropy** atau **information gain** terbesar.

$$Information \ Gain = Entropy(parent) - (P_1 \  Entropy_1 + P_2 \  Entropy_2)$$

-   $P_1$: proporsi subset data kelas pertama, misalkan `Budget = High`
-   $P_2$: proporsi subset data kelas kedua, misalkan `Budget = Low`
-   $Entropy_1$: entropy untuk subset data kelas pertama, misalkan
    `Budget = High`
-   $Entropy_2$: entropy untuk subset data kelas kedua, misalkan
    `Budget = Low`

1.  Information Gain apabila splitting dengan `Budget`:

```{r}
table(budget = dine$Budget, dine = dine$Dine.Out)
```

```{r}
entropy_budget_high <- - 2/6*log2(2/6) - 4/6*log2(4/6)
entropy_budget_low <- - 6/6*log2(6/6)

entropy_budget <- 6/12 * entropy_budget_high +
                  6/12 * entropy_budget_low
entropy_budget
```

```{r}
ig_budget <- entropy_awal - entropy_budget
ig_budget
```

2.  Information Gain apabila splitting dengan `Distance`:

```{r}
table(distance = dine$Distance, dine = dine$Dine.Out)
```

```{r}
entropy_distance_far <- - 5/6*log2(5/6) - 1/6*log2(1/6)
entropy_distance_near <- - 3/6*log2(3/6) - 3/6*log2(3/6) 

entropy_distance <- 6/12 * entropy_distance_far +
                    6/12 * entropy_distance_near
entropy_distance
```

```{r}
ig_distance <- entropy_awal - entropy_distance
ig_distance
```

3.  Information Gain apabila splitting dengan `Friend`:

```{r}
table(friend = dine$Friend, dine = dine$Dine.Out)
```

```{r}
entropy_friend_absent <- - 5/7*log2(5/7) - 2/7*log2(2/7)
entropy_friend_available <- - 3/5*log2(3/5) - 2/5*log2(2/5) 

entropy_friend <- 7/12 * entropy_friend_absent +
                  5/12 * entropy_friend_available
entropy_friend
```

```{r}
ig_friend <- entropy_awal - entropy_friend
ig_friend
```

4.  Penentuan variable manakah yang menghasilkan information gain
    terbesar.

```{r}
ig_budget
ig_distance
ig_friend
```

Ternyata information gain ketika menggunakan predictor `Budget` paling
besar, maka predictor tersebut dijadikan root node. Untuk menentukan
internal node selanjutnya, dilakukan perhitungan information gain yang
serupa.

## Study Case: Diabetes

**Business Question:** Sebagai seorang konsultan kesehatan di sebuah
rumah sakit, kita diminta untuk membuat rules dari hasil tes diagnosis
dan riwayat penyakit diabetes dari 768 pasien. Dalam kasus ini, kita
akan mengaplikasikan decision tree agar rules dapat disajikan dalam
bentuk visualisasi yang mudah diinterpretasi.

-   kelas positif: diabetes (pos)
-   kelas negatif: sehat (neg)

### Read Data

```{r}
# read data
diab <- read.csv("data_input/diabetes.csv")

# cek data
head(diab)
```

Deskripsi data:

-   `pregnant`: Number of times pregnant
-   `glucose`: Plasma glucose concentration (glucose tolerance test)
-   `pressure`: Diastolic blood pressure (mm Hg)
-   `triceps`: Triceps skin fold thickness (mm)
-   `insulin`: 2-Hour serum insulin (mu U/ml)
-   `mass`: Body mass index (weight in kg/(height in m)\^2)
-   `pedigree`: Diabetes pedigree function
-   `age`: Age (years)
-   `diabetes`: Test for Diabetes

### Data Wrangling

Sesuaikan tipe data pada dataset `diab`:

```{r}
# your code

```

### Cross-Validation

Split data `diab` menjadi `diab_train` dan `diab_test` dengan proporsi
80:20

```{r}
library(rsample)
RNGkind(sample.kind = "Rounding")
set.seed(100)

```

Cek proporsi kelas di data training:

```{r}

```

Apakah perlu upsampling/downsampling?

Apabila diperhatikan, proporsi `diab_train` sebenarnya masih bisa
dianggap balance. Namun kita bisa coba membuatnya lebih balance lagi
sehingga proporsinya 50:50, kita kenal dengan istilah:

-   **Upsample** = menambahkan observasi minoritas hingga seimbang
    dengan mayoritas, dengan menduplikat data dari observasi minoritas.
    memiliki kekurangan dimana data tambahannya sebenarnya hanya berupa
    duplikat (tidak menambah informasi baru).
-   **Downsample** = mengurangi observasi mayoritas hingga seimbang
    dengan yang minoritas. memiliki kekurangan dimana kita akan membuang
    beberapa informasi dari data yg dimiliki, umum digunakan bila kita
    memiliki data yang cukup banyak.

**Note:** *data yang cukup banyak* dalam artian frekuensi datanya cukup
banyak untuk kelas minoritas. Ini bisa dilihat dari
`table(data$kolom_target)`. Kalau ternyata terdapat 1000 data namun
ternyata 800 kelas mayoritas, dan 200 kelas minoritas, setelah
downsampling jadi ada 400 data, datanya masih cukup sedikit. Usahakan
jumlah data minoritas setidaknya 300 observasi.

**More Note:** Bila data masih terlalu sedikit dan tidak ingin melakukan
upsampling, berarti kita harus menambah data.

```{r}
# downsampling
RNGkind(sample.kind = "Rounding")
set.seed(100)

library(caret)

```

```{r}

```

**Note:** Lakukan penyeimbangan target class hanya pada **training
data**. Namun tidak dilakukan pada testing data, perlakukanlah sebagai
unseen data.

## Model Fitting

```{r}
library(partykit)

```

```{r}
# print struktur pohon

```

```{r fig.width=10}
# visualisasi decision tree

```

## Model Evaluation

Mari evaluasi `diabetes_tree` menggunakan confusion matrix berdasarkan
hasil prediksi di data test:

Parameter `type`:

-   `type = "prob"` mengembalikan nilai peluang untuk masing-masing
    kelas
-   `type = "response"` mengembalikan label kelasnya (default threshold
    0.5)

```{r}
# prediksi kelas di data test


# confusion matrix data test

```

Meminimalisir: False Negative

Metric apa yang kita perhatikan pada kasus ini? Dengan ketentuan,
apabila pasien dideteksi sebagai positif diabetes maka diarahkan untuk
test dan konsultasi lanjutan.

-   ...

Selain melihat confusion matrix untuk data test saja, ada baiknya kita
juga bandingkan dengan performance model di data train (`diab_train`)
untuk mengetahui fitting dari sebuah model:

```{r}
# prediksi kelas di data train

# confusion matrix data train

```

-   Recall di data train:
-   Recall di data test:

Ketika dibandingkan, performa model `diabetes_tree` cenderung lebih baik
pada data train dibandingkan data test. Model ini dikatakan sebagai
model yang **overfitting**. Karena performa model belum cukup baik,
perlu dilakukan *model tuning*.

# üìù Summary Day 2

1.  Naive Bayes sering digunakan untuk kasus text classification, karena
    waktu komputasi cukup cepat untuk predictor kata yang banyak.
2.  Workflow pada text mining adalah:

<!-- -->

a.  Read data dan wrangling
b.  Eksplorasi data
c.  Text cleansing
d.  Tokenization (DocumentTermMatrix)
e.  Cross-Validation
f.  Further Preprocessing: i. Feature selection `findFreqTerms()` ii.
    Bernoulli converter
g.  Train model dan prediksi
h.  Evaluasi model

<!-- -->

3.  Karakteristik Decision Tree:

-   Mudah diinterpretasi (melalui visualisasi)
-   Robust (performa model stabil)
-   Antar predictor diasumsikan dependent

4.  Decision Tree membuat sebuah pohon keputusan dengan memilih
    prediktor yang menghasilkan splitting data yang homogen atau
    **entropy**nya kecil Dengan kata lain, prediktor tersebut
    menghasilkan **information gain** yang besar.
